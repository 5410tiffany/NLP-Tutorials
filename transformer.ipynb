{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese time order: yy/mm/dd  ['31-04-26', '04-07-18', '33-06-06'] \n",
      "English time order: dd/M/yyyy  ['26/Apr/2031', '18/Jul/2004', '06/Jun/2033']\n",
      "vocabularies:  {'2', '5', 'Oct', '<EOS>', 'Mar', 'May', 'Dec', '7', 'Jan', '1', 'Jul', 'Sep', '3', 'Jun', 'Apr', '8', '4', 'Aug', '0', '9', '<GO>', '-', '<PAD>', '/', '6', 'Nov', 'Feb'}\n",
      "x index sample: \n",
      "31-04-26\n",
      "[6 4 1 3 7 1 5 9] \n",
      "y index sample: \n",
      "<GO>26/Apr/2031<EOS>\n",
      "[14  5  9  2 15  2  5  3  6  4 13]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow._api.v1.initializers' has no attribute 'RandomNormal'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dbddd99cd9e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    267\u001b[0m           \"\\ny index sample: \\n{}\\n{}\".format(d.idx2str(d.y[0]), d.y[0]))\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_LAYER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_HEAD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDROP_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0mexport_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dbddd99cd9e3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_dim, max_len, n_layer, n_head, n_vocab, drop_rate, padding_idx)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPositionEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dbddd99cd9e3>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_len, model_dim, n_vocab)\u001b[0m\n\u001b[1;32m    135\u001b[0m         self.embeddings = keras.layers.Embedding(\n\u001b[1;32m    136\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_dim\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# [n_vocab, dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0membeddings_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow_core/python/util/module_wrapper.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    191\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m       \u001b[0mattr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_wrapped_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfmw_public_apis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v1.initializers' has no attribute 'RandomNormal'"
     ]
    }
   ],
   "source": [
    "# [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import utils    # this refers to utils.py in my [repo](https://github.com/MorvanZhou/NLP-Tutorials/)\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "MODEL_DIM = 32\n",
    "MAX_LEN = 12\n",
    "N_LAYER = 3\n",
    "N_HEAD = 4\n",
    "DROP_RATE = 0.1\n",
    "\n",
    "\n",
    "class MultiHead(keras.layers.Layer):\n",
    "    def __init__(self, n_head, model_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        self.head_dim = model_dim // n_head\n",
    "        self.n_head = n_head\n",
    "        self.model_dim = model_dim\n",
    "        self.wq = keras.layers.Dense(n_head * self.head_dim)\n",
    "        self.wk = keras.layers.Dense(n_head * self.head_dim)\n",
    "        self.wv = keras.layers.Dense(n_head * self.head_dim)      # [n, step, h*h_dim]\n",
    "\n",
    "        self.o_dense = keras.layers.Dense(model_dim)\n",
    "        self.o_drop = keras.layers.Dropout(rate=drop_rate)\n",
    "        self.attention = None\n",
    "\n",
    "    def call(self, q, k, v, mask, training):\n",
    "        _q = self.wq(q)      # [n, q_step, h*h_dim]\n",
    "        _k, _v = self.wk(k), self.wv(v)     # [n, step, h*h_dim]\n",
    "        _q = self.split_heads(_q)  # [n, h, q_step, h_dim]\n",
    "        _k, _v = self.split_heads(_k), self.split_heads(_v)  # [n, h, step, h_dim]\n",
    "        context = self.scaled_dot_product_attention(_q, _k, _v, mask)     # [n, q_step, h*dv]\n",
    "        o = self.o_dense(context)       # [n, step, dim]\n",
    "        o = self.o_drop(o, training=training)\n",
    "        return o\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        x = tf.reshape(x, (x.shape[0], x.shape[1], self.n_head, self.head_dim))  # [n, step, h, h_dim]\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])       # [n, h, step, h_dim]\n",
    "\n",
    "    def scaled_dot_product_attention(self, q, k, v, mask=None):\n",
    "        dk = tf.cast(k.shape[-1], dtype=tf.float32)\n",
    "        score = tf.matmul(q, k, transpose_b=True) / (tf.math.sqrt(dk) + 1e-8)  # [n, h_dim, q_step, step]\n",
    "        if mask is not None:\n",
    "            score += mask * -1e9\n",
    "        self.attention = tf.nn.softmax(score, axis=-1)                               # [n, h, q_step, step]\n",
    "        context = tf.matmul(self.attention, v)         # [n, h, q_step, step] @ [n, h, step, dv] = [n, h, q_step, dv]\n",
    "        context = tf.transpose(context, perm=[0, 2, 1, 3])   # [n, q_step, h, dv]\n",
    "        context = tf.reshape(context, (context.shape[0], context.shape[1], -1))     # [n, q_step, h*dv]\n",
    "        return context\n",
    "\n",
    "\n",
    "class PositionWiseFFN(keras.layers.Layer):\n",
    "    def __init__(self, model_dim):\n",
    "        super().__init__()\n",
    "        dff = model_dim * 4\n",
    "        self.l = keras.layers.Dense(dff, activation=keras.activations.relu)\n",
    "        self.o = keras.layers.Dense(model_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        o = self.l(x)\n",
    "        o = self.o(o)\n",
    "        return o         # [n, step, dim]\n",
    "\n",
    "\n",
    "class EncodeLayer(keras.layers.Layer):\n",
    "    def __init__(self, n_head, model_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        self.ln = [keras.layers.LayerNormalization(axis=-1) for _ in range(2)]  # only norm z-dim\n",
    "        self.mh = MultiHead(n_head, model_dim, drop_rate)\n",
    "        self.ffn = PositionWiseFFN(model_dim)\n",
    "        self.drop = keras.layers.Dropout(drop_rate)\n",
    "\n",
    "    def call(self, xz, training, mask):\n",
    "        attn = self.mh.call(xz, xz, xz, mask, training)       # [n, step, dim]\n",
    "        o1 = self.ln[0](attn + xz)\n",
    "        ffn = self.drop(self.ffn.call(o1), training)\n",
    "        o = self.ln[1](ffn + o1)         # [n, step, dim]\n",
    "        return o\n",
    "\n",
    "\n",
    "class Encoder(keras.layers.Layer):\n",
    "    def __init__(self, n_head, model_dim, drop_rate, n_layer):\n",
    "        super().__init__()\n",
    "        self.ls = [EncodeLayer(n_head, model_dim, drop_rate) for _ in range(n_layer)]\n",
    "\n",
    "    def call(self, xz, training, mask):\n",
    "        for l in self.ls:\n",
    "            xz = l.call(xz, training, mask)\n",
    "        return xz       # [n, step, dim]\n",
    "\n",
    "\n",
    "class DecoderLayer(keras.layers.Layer):\n",
    "    def __init__(self, n_head, model_dim, drop_rate):\n",
    "        super().__init__()\n",
    "        self.ln = [keras.layers.LayerNormalization(axis=-1) for _ in range(3)] # only norm z-dim\n",
    "        self.drop = keras.layers.Dropout(drop_rate)\n",
    "        self.mh = [MultiHead(n_head, model_dim, drop_rate) for _ in range(2)]\n",
    "        self.ffn = PositionWiseFFN(model_dim)\n",
    "\n",
    "    def call(self, yz, xz, training, yz_look_ahead_mask, xz_pad_mask):\n",
    "        attn = self.mh[0].call(yz, yz, yz, yz_look_ahead_mask, training)       # decoder self attention\n",
    "        o1 = self.ln[0](attn + yz)\n",
    "        attn = self.mh[1].call(o1, xz, xz, xz_pad_mask, training)       # decoder + encoder attention\n",
    "        o2 = self.ln[1](attn + o1)\n",
    "        ffn = self.drop(self.ffn.call(o2), training)\n",
    "        o = self.ln[2](ffn + o2)\n",
    "        return o\n",
    "\n",
    "\n",
    "class Decoder(keras.layers.Layer):\n",
    "    def __init__(self, n_head, model_dim, drop_rate, n_layer):\n",
    "        super().__init__()\n",
    "        self.ls = [DecoderLayer(n_head, model_dim, drop_rate) for _ in range(n_layer)]\n",
    "\n",
    "    def call(self, yz, xz, training, yz_look_ahead_mask, xz_pad_mask):\n",
    "        for l in self.ls:\n",
    "            yz = l.call(yz, xz, training, yz_look_ahead_mask, xz_pad_mask)\n",
    "        return yz\n",
    "\n",
    "\n",
    "class PositionEmbedding(keras.layers.Layer):\n",
    "    def __init__(self, max_len, model_dim, n_vocab):\n",
    "        super().__init__()\n",
    "        pos = np.arange(max_len)[:, None]\n",
    "        pe = pos / np.power(10000, 2. * np.arange(model_dim)[None, :] / model_dim)  # [max_len, dim]\n",
    "        pe[:, 0::2] = np.sin(pe[:, 0::2])\n",
    "        pe[:, 1::2] = np.cos(pe[:, 1::2])\n",
    "        pe = pe[None, :, :]  # [1, max_len, model_dim]    for batch adding\n",
    "        self.pe = tf.constant(pe, dtype=tf.float32)\n",
    "        self.embeddings = keras.layers.Embedding(\n",
    "            input_dim=n_vocab, output_dim=model_dim,  # [n_vocab, dim]\n",
    "            embeddings_initializer=tf.initializers.RandomNormal(0., 0.01),\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x_embed = self.embeddings(x) + self.pe  # [n, step, dim]\n",
    "        return x_embed\n",
    "\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(self, model_dim, max_len, n_layer, n_head, n_vocab, drop_rate=0.1, padding_idx=0):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embed = PositionEmbedding(max_len, model_dim, n_vocab)\n",
    "        self.encoder = Encoder(n_head, model_dim, drop_rate, n_layer)\n",
    "        self.decoder = Decoder(n_head, model_dim, drop_rate, n_layer)\n",
    "        self.o = keras.layers.Dense(n_vocab)\n",
    "\n",
    "        self.cross_entropy = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=\"none\")\n",
    "        self.opt = keras.optimizers.Adam(0.002)\n",
    "\n",
    "    def call(self, x, y, training=None):\n",
    "        x_embed, y_embed = self.embed(x), self.embed(y)\n",
    "        pad_mask = self._pad_mask(x)\n",
    "        encoded_z = self.encoder.call(x_embed, training, mask=pad_mask)\n",
    "        decoded_z = self.decoder.call(\n",
    "            y_embed, encoded_z, training, yz_look_ahead_mask=self._look_ahead_mask(y), xz_pad_mask=pad_mask)\n",
    "        o = self.o(decoded_z)\n",
    "        return o\n",
    "\n",
    "    def step(self, x, y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.call(x, y[:, :-1], training=True)\n",
    "            pad_mask = tf.math.not_equal(y[:, 1:], self.padding_idx)\n",
    "            loss = tf.reduce_mean(tf.boolean_mask(self.cross_entropy(y[:, 1:], logits), pad_mask))\n",
    "        grads = tape.gradient(loss, self.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.trainable_variables))\n",
    "        return loss, logits\n",
    "\n",
    "    def _pad_bool(self, seqs):\n",
    "        return tf.math.equal(seqs, self.padding_idx)\n",
    "\n",
    "    def _pad_mask(self, seqs):\n",
    "        mask = tf.cast(self._pad_bool(seqs), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]  # (n, 1, 1, step)\n",
    "\n",
    "    def _look_ahead_mask(self, seqs):\n",
    "        mask = 1 - tf.linalg.band_part(tf.ones((self.max_len, self.max_len)), -1, 0)\n",
    "        mask = tf.where(self._pad_bool(seqs)[:, tf.newaxis, tf.newaxis, :], 1, mask[tf.newaxis, tf.newaxis, :, :])\n",
    "        return mask  # (step, step)\n",
    "\n",
    "    def translate(self, src, v2i, i2v):\n",
    "        src_pad = utils.pad_zero(src, self.max_len)\n",
    "        tgt = utils.pad_zero(np.array([[v2i[\"<GO>\"], ] for _ in range(len(src))]), self.max_len+1)\n",
    "        tgti = 0\n",
    "        x_embed = self.embed(src_pad)\n",
    "        encoded_z = self.encoder.call(x_embed, False, mask=self._pad_mask(src_pad))\n",
    "        while True:\n",
    "            y = tgt[:, :-1]\n",
    "            y_embed = self.embed(y)\n",
    "            decoded_z = self.decoder.call(\n",
    "                y_embed, encoded_z, False, yz_look_ahead_mask=self._look_ahead_mask(y), xz_pad_mask=self._pad_mask(src_pad))\n",
    "            logits = self.o(decoded_z)[:, tgti, :].numpy()\n",
    "            idx = np.argmax(logits, axis=1)\n",
    "            tgti += 1\n",
    "            tgt[:, tgti] = idx\n",
    "            if tgti >= self.max_len:\n",
    "                break\n",
    "        return [\"\".join([i2v[i] for i in tgt[j, 1:tgti]]) for j in range(len(src))]\n",
    "\n",
    "    @property\n",
    "    def attentions(self):\n",
    "        attentions = {\n",
    "            \"encoder\": [l.mh.attention.numpy() for l in self.encoder.ls],\n",
    "            \"decoder\": {\n",
    "                \"mh1\": [l.mh[0].attention.numpy() for l in self.decoder.ls],\n",
    "                \"mh2\": [l.mh[1].attention.numpy() for l in self.decoder.ls],\n",
    "        }}\n",
    "        return attentions\n",
    "\n",
    "\n",
    "def train(model, data, step):\n",
    "    # training\n",
    "    t0 = time.time()\n",
    "    for t in range(step):\n",
    "        bx, by, seq_len = data.sample(64)\n",
    "        bx, by = utils.pad_zero(bx, max_len=MAX_LEN), utils.pad_zero(by, max_len=MAX_LEN + 1)\n",
    "        loss, logits = model.step(bx, by)\n",
    "        if t % 50 == 0:\n",
    "            logits = logits[0].numpy()\n",
    "            t1 = time.time()\n",
    "            print(\n",
    "                \"step: \", t,\n",
    "                \"| time: %.2f\" % (t1 - t0),\n",
    "                \"| loss: %.4f\" % loss.numpy(),\n",
    "                \"| target: \", \"\".join([data.i2v[i] for i in by[0, 1:10]]),\n",
    "                \"| inference: \", \"\".join([data.i2v[i] for i in np.argmax(logits, axis=1)[:10]]),\n",
    "            )\n",
    "            t0 = t1\n",
    "\n",
    "    os.makedirs(\"./visual/models/transformer\", exist_ok=True)\n",
    "    model.save_weights(\"./visual/models/transformer/model.ckpt\")\n",
    "    os.makedirs(\"./visual/tmp\", exist_ok=True)\n",
    "    with open(\"./visual/tmp/transformer_v2i_i2v.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"v2i\": data.v2i, \"i2v\": data.i2v}, f)\n",
    "\n",
    "\n",
    "def export_attention(model, data, name=\"transformer\"):\n",
    "    with open(\"./visual/tmp/transformer_v2i_i2v.pkl\", \"rb\") as f:\n",
    "        dic = pickle.load(f)\n",
    "    model.load_weights(\"./visual/models/transformer/model.ckpt\")\n",
    "    bx, by, seq_len = data.sample(32)\n",
    "    model.translate(bx, dic[\"v2i\"], dic[\"i2v\"])\n",
    "    attn_data = {\n",
    "        \"src\": [[data.i2v[i] for i in bx[j]] for j in range(len(bx))],\n",
    "        \"tgt\": [[data.i2v[i] for i in by[j]] for j in range(len(by))],\n",
    "        \"attentions\": model.attentions}\n",
    "    path = \"./visual/tmp/%s_attention_matrix.pkl\" % name\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(attn_data, f)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    utils.set_soft_gpu(True)\n",
    "    d = utils.DateData(4000)\n",
    "    print(\"Chinese time order: yy/mm/dd \", d.date_cn[:3], \"\\nEnglish time order: dd/M/yyyy \", d.date_en[:3])\n",
    "    print(\"vocabularies: \", d.vocab)\n",
    "    print(\"x index sample: \\n{}\\n{}\".format(d.idx2str(d.x[0]), d.x[0]),\n",
    "          \"\\ny index sample: \\n{}\\n{}\".format(d.idx2str(d.y[0]), d.y[0]))\n",
    "\n",
    "    m = Transformer(MODEL_DIM, MAX_LEN, N_LAYER, N_HEAD, d.num_word, DROP_RATE)\n",
    "    train(m, d, step=800)\n",
    "    export_attention(m, d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
